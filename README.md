# RobotsTxtFinder ü§ñ

# English üá¨üáß

# Description

This tool allows for a quick check of whether the robots.txt files are accessible for a given set of URLs. It takes as input a text file containing the URLs to be checked, as well as various customizable parameters such as the maximum number of concurrent requests, the maximum timeout for each request, and the choice of User-Agent. Accessible URLs with robots.txt are saved to a text file as output. 
The script adds the "/robots.txt" path to the end of each URL and prepends the "http" protocol to the beginning before making the request.
The script uses asyncio to allow for asynchronous URL checking, but an option is also available to switch to synchronous mode.

# Installation :

  - Clone this repo :
  `git clone https://github.com/lhommedelasituation/RobotsTxtFinder/`
  
  - Change the working directory to RobotsTxtFinder :
  `cd RobotsTxtFinder`
  
  - Install requirements.txt
  `pip install -r requirements.txt`
  
# Usage :

  - Before using, create .txt file with urls or domain name to ckeck.
  - Run this command :
  `python robotstxtfinder.py /path/to/urls_file.txt --user-agent "my_user_agent" --concurrency 20 --timeout 10 --async`
  
# Fran√ßais üá´üá∑

# Description

Cet outil permet de v√©rifier rapidement si les fichiers robots.txt sont accessibles pour un ensemble d'URLs donn√©. Il prend en entr√©e un fichier texte contenant les URLs √† v√©rifier, ainsi que divers param√®tres personnalisables tels que le nombre maximal de requ√™tes simultan√©es, le d√©lai d'attente maximal pour chaque requ√™te et le choix du User-Agent. Les URLs accessibles sont sauvegard√©es dans un fichier texte en sortie.
Le script ajoute le chemin "/robots.txt" √† la fin de chaque URL et ajoute le protocole "http" au d√©but avant de faire la requ√™te.
Le script utilise asyncio pour permettre une v√©rification asynchrone des URLs, mais une option est √©galement disponible pour passer en mode synchrone.

# Installation :

  - Clonez ce repo :
  `git clone https://github.com/lhommedelasituation/RobotsTxtFinder/`

  - Changez le r√©pertoire de travail en RobotsTxtFinder :
  `cd RobotsTxtFinder`

  - Installez requirements.txt :
  `pip install -r requirements.txt`
  
# Utilisation :

  - Avant d'utiliser l'outil, cr√©ez un fichier .txt avec les URLs ou les noms de domaine √† v√©rifier.
  - Ex√©cutez la commande suivante :
  `python robotstxtfinder.py /chemin/vers/fichier_urls.txt --user-agent "mon_user_agent" --concurrency 20 --timeout 10 --async`
